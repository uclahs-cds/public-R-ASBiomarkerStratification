---
title: 'Active Surveillance Progression Prediction'
author: 'Stefan Eng'
output:
  bookdown::pdf_document2:
    template: boutros_template.tex
    keep_tex: true
  html_notebook: default
bibliography:  bibliography.bib
csl: nature-no-superscript.csl
---

```{r setup, include=FALSE}
library(ProstateCancer.ASBiomarkerSynergy);
library(BoutrosLab.plotting.general);
library(kableExtra);
library(gridExtra);
library(magrittr);
library(here);
library(caret);
library(pROC);
library(rpart.plot);

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.align = 'center', out.width = '60%');

axis.cex <- .7
lab.cex <- .8
strip.cex <- 0.45

km.arguments <- list(
  xaxis.cex = axis.cex,
  yaxis.cex = axis.cex,
  main.cex = 1.4,
  xlab.cex = lab.cex,
  ylab.label = 'Estimated survival probability',
  ylab.cex = lab.cex,
  risktable.fontsize = 14,
  key.groups.cex = 1,
  key.stats.cex = 1,
  key.stats.y.pos = 0.5
  );

as.data <- default.load.data();
biodb <- as.data$biodb;

valid.patients <- biodb$NoUpgradeAndProgressed == 0 | is.na(biodb$NoUpgradeAndProgressed);

biodb.valid <- biodb[valid.patients, ];
```

# Background (To Remove)


In general a patient with GS 3+3 (now increasingly called ISUP Grade Group 1 or ISUP GG1 or just GG1) will go on AS while a 3+4 (GG2) may but probably will not and a 4+3 (gg3) definitely will not.  Thus you can also look at the patients who did get surgery and see if a GG1 patient on biopsy (a tiny fraction of the tumor) was actually GG2 or GG3 (or worse) after surgery when the entire tumor is available for checking.  Thus looking at the surgical pathology can remove the spatial variability component (called undersampling) of the biopsy procedure out.

Clinically, AS means *not* treating a patient.  This is of course statistically superior for a bulk population, but still leads to problems when there is a FN (somebody has aggressive disease, but that isn’t recognized and thus they are inappropriately on AS).  Many patients will therefore voluntarily elect to exit AS prematurely, seeking a treatment that they may not derive benefit from.  Thus the key clinical problem is unrecognized aggressive disease, which both hinders those patients directly and more broadly reduces confidence in AS.  The goal of our work is thus to give better identification of which patients should exit AS, so that those who should not have more confidence in their decision.  Surgery/Radiotherapy (Sx/Rt) is extremely expensive ($20-50k) and has long-term morbidities (quality-of-life impacts).  We sometimes talk about this in terms of “health preference values”, which reflects how much a change in life-quality relates to a full-year of health.  Very roughly, every year on AS relative to definitive local therapy (Sx or Rt) saves the patient ~0.2 years of fully-healthy life, so prolonging AS as long as possible has big advantages.  For example, my dad has been on AS for almost 7 years now, and it’s very possible he will be on it for the rest of his life.  But, his initial immediate instinct was “cut this thing out of me”, and having a son who could walk him through the decision-making made a big difference there.  So in the long-run, the question is really “can these biomarkers improve confidence men have to remain on AS” as much as anything else -- ~15% each year *voluntarily* leave AS for treatment.

# Introduction

## Study design and objectives
  There is a clinical need to predict *before* surgery if a man has aggressive disease so that we can decide if they need surgery at all.  The quality of life benefits are thus huge (avoiding therapy entirely!).  The problem is that because it’s pre-surgery, we do not have the full cancer to study.  Instead we use biopsies (spatially-restricted samples of the cancer), radiology (imaging like MRI) and minimally-invasive biomarkers (e.g. blood or urine tests).  It's unclear which of those different strategies is best, and how those strategies should be sequenced or ordered. A collaborator at UTHSCSA (University of Texas Health Sciences Center San Antonio, I think) Dr. Michael Liss is a surgeon who is thinking hard about these problems.  He's put together a really nice cohort of ~100 patients where basically every possible biomarker has been generated and we want to figure out 'what is the optimal biomarker we can make using all tests'.  That will put an upper-bound to accuracy which we can go investigate in a prospective clinical trial.  We can then go backwards to start figuring out if there are ways to simplify/cheapen that test.
  
## Data description
In this cohort there are $N = `r nrow(biodb)`$ patients.
There are three variables that are of interest to predict.
The first, `BiopsyUpgraded`, is whether the patient's research study biopsy increased cancer grade from the most recent biopsy results.
The second, `ProgressedToTreatment`, is whether the patient progressed to treatment.
Finally we have `Prostatectomy` which indicates whether the patient had a prostatectomy.

The Prostate Health Index (PHI) and PHI density have been used in detection of prostate cancer [@Druskin2017].

The biomarkers used are shown in \@ref(tab:biomarker-categories).
```{r biomarker-categories}
kable(as.data$bio.categories[,1:2], caption = "Biomarkers categories") %>%
  kable_styling() %>%
  collapse_rows(columns = 1)
```

Along with these biomarkers, we computed the PSA Density as `PSAHyb / ProstateVolume` and the PHI Density as `PHI / ProstateVolume`. 

```{r prediction-vars}
pred.vars.xtabs <- xtabs(~ BiopsyUpgraded + ProgressedToTreatment + Prostatectomy, data = biodb, addNA = TRUE)
pred.vars.df <- as.data.frame(ftable(pred.vars.xtabs))

pred.table <- reshape(pred.vars.df, idvar = c('BiopsyUpgraded', 'ProgressedToTreatment'), timevar = 'Prostatectomy', direction = 'wide')

row.order <- with(pred.table, order(BiopsyUpgraded, ProgressedToTreatment))
prostatectomy.cols <- paste0(c('no', 'yes'), ' (N=', table(biodb$Prostatectomy), ')')
overall.col <- paste0('Overall (N=', nrow(biodb), ')')

pred.table$Overall <- rowSums(pred.table[row.order, c(3,4)])

pred.table[row.order, ] %>%
  kable(row.names = FALSE,
    booktabs = TRUE,
    align =  c('c', 'c', 'c', 'c'),
    col.names = c('Biopsy Upgraded', 'Progressed To Treatment', prostatectomy.cols, overall.col),
    caption = 'Comparison of prediction variables of interest') %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(' ' = 2, 'Prostatectomy' = 2, ' ' = 1), align = 'c')
```

The severity of the prostate cancers were measured on the Gleason scale/ISUP grade group [@ISUP2016].

Gleason Scores | Grade Group
--- | ---
Gleason Score $\leq$ 6 | Grade Group 1
3 + 4 = 7 | Grade Group 2
4 + 3 = 7 | Grade Group 3
Gleason Score 8 | Grade Group 4
Gleason Score 9-10 | Grade Group 5

The variable `PreviousISUP` is the most recent prostate cancer ISUP grade group (GG) rating prior to research biopsy.
The ISUP grade group is computed again in the variable `StudyHighestISUP` after the research MRI.
We compare the progression of the cancers by the Gleason grades.
```{r}
progressed <- rep(NA, length(biodb$PreviousISUP))
progressed[biodb$PreviousISUP < biodb$StudyHighestISUP] <- 'Increased';
progressed[biodb$PreviousISUP > biodb$StudyHighestISUP] <- 'Decreased';
progressed[biodb$PreviousISUP == biodb$StudyHighestISUP] <- 'No Change';

kable(t(table(progressed)), caption = 'Comparison of pre-research biopsy ISUP grade group to post research MRI ISUP GG');

isup.names <- c('No Cancer', 1:5, 'NA');

isup.table <- with(biodb, table(PreviousISUP, StudyHighestISUP, useNA='ifany'))
row.names(isup.table) <- isup.names;

isup.table %>%
  kable(
    booktabs = TRUE,
    align =  'c',
    col.names = isup.names
    ) %>%
  kable_styling() %>%
  add_header_above(c(' ' = 1, 'Study Highest ISUP Grade Group ' = length(levels(biodb$StudyHighestGleason)) + 1), align = 'c')
```
# Results

## Descriptive Statistics

### Demographics

```{r, results= 'hide', out.width='90%', fig.cap='Demographics by race and whether biopsy was upgraded.'}
demographic.vars <- c('Age', 'Weight', 'Height', 'BMI');
bx.plots <- lapply(demographic.vars, demographics.boxplot, biodb = biodb, x = 'BiopsyUpgraded', cond = 'Race', yaxis.cex = axis.cex, xaxis.cex = axis.cex, ylab.cex = lab.cex, xlab.cex = lab.cex, strip.cex = strip.cex);
do.call(gridExtra::grid.arrange, bx.plots);
```

```{r, results= 'hide', out.width='90%', fig.cap='Demographics by race and whether patient progressed to treatment.'}
demographic.vars <- c('Age', 'Weight', 'Height', 'BMI');
prog.plots <- lapply(demographic.vars, demographics.boxplot, biodb = biodb, x = 'ProgressedToTreatment', cond = 'Race', yaxis.cex = axis.cex, xaxis.cex = axis.cex, ylab.cex = lab.cex, xlab.cex = lab.cex, strip.cex = strip.cex);
do.call(gridExtra::grid.arrange, prog.plots);
```

```{r}
tab.form <- "~ p2PSA +  freePSA + PSAHyb + PSADensity + SOCPSA + PHI + PHIDensity + ProstateVolume + TNFaAverage"
# TODO: Update render.default to always include missing?
t1 <- table1::table1(as.formula(paste(tab.form, 'ProgressedToTreatment', sep="|")), data = biodb);
table1.to.kable(t1) %>%
  add_header_above(c(' ' = 1, 'Progressed To Treatment' = 2, ' ' = 1))
```

```{r}
t1.bx <- table1::table1(as.formula(paste(tab.form, 'BiopsyUpgraded', sep="|")), data = biodb);
table1.to.kable(t1.bx) %>%
  add_header_above(c(' ' = 1, 'Biopsy Upgraded' = 2, ' ' = 1))
```

```{r, out.width = '100%', fig.cap='Correlation heapmap with test with test methodology labels.'}
create.heatmap.AS(biodb);
```

### Time-to-event data
```{r}
stopifnot(
    all(!is.na(biodb$DaysDxToLastClinicalAppt))
    );
stopifnot(
    all(!is.na(biodb$DaysDxToLastReview))
    );
stopifnot(
    all(is.na(biodb$BiopsyResult) == is.na(biodb$DaysBxToLastReview))
    );
stopifnot(
    all(is.na(biodb$BiopsyResult) == is.na(biodb$DaysBxToLastClinicalAppt))
    );
```
All patients have values for the days from diagnosis to the last clinical appointment `DaysDxToLastClinicalAppt` and last review `DaysDxToLastReview`.
The patients that had a biopsy have values for variables `DaysBxToLastClinicalAppt` and `DaysBxToLastReview`.
If a patient progressed to treatment, then the days from diagnosis are in `DaysDxToProgression`.
If they had a biopsy and progressed to treatment then `DaysDxToProgression` will contain the days between the biopsy and progression.

#### Overall Days to Progression from Biopsy/Diagnosis

```{r days-bx-progression, fig.cap='Days to progression from biopsy for entire cohort.'}
# Create a Surv object from survival package
progression.surv.bx <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysBxToProgression, biodb$DaysBxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.bx,
      main = 'Overall Days-to-Progression from Biopsy',    
      xlab.label = 'Time to progression from biopsy (Days)'
      )
    )
  )
```

```{r }
progression.surv.dx <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysDxToProgression, biodb$DaysDxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.dx,
      main = 'Overall Days-to-Progression from Diagnosis',
      xlab.label = 'Time to progression from diagnosis (Days)'
      )
    )
  )
```

#### Overall Time-to-Upgrade from Diagnosis
```{r}
# Create a Surv object from survival package
upgrade.surv <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysDxToUpgrade, biodb$DaysDxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = upgrade.surv,
      main = 'Overall Days-to-Upgrade from Diagnosis',
      xlab.label = 'Time to upgrade since diagnosis (Days)'
      )
    )
  )
```

## Predictive Analysis
```{r}
# seed <- 1313;
seed <- 42;
model.names <- c('rpart', 'gbm', 'xgb');
targets <- c(
    'BiopsyUpgraded',
    'Prostatectomy',
    'ProgressedToTreatment');
metric <- 'PR-AUC';

models <- lapply(targets, function(tg) {
    model.file.names <- here(paste0('models/', model.names, '_', tg, '_', metric, '_', seed, '_model.RDS'));
    res <- lapply(model.file.names, readRDS);
    names(res) <- model.names;
    res
});
names(models) <- targets;
```

We predict the three variables: `ProgressedToTreatment`, `BiopsyUpgraded`, `Prostatectomy`.
For each of these variable we compare the results from three models: a regression tree (`rpart`), gradient boosting machine (`gbm`), and eXtreme Gradient Boosting (`xgboost`).
We only report the results from `rpart` and `gbm` as the results are very similar for the two gradient boosting methods and `gbm` performs slightly better.
The regression tree model is the most interpretable as it produces a single decision tree.
The other models produce better accuracy as well as $F_1$ scores but require more work to interpret.

All of the models were validated using 10-fold cross-validation repeated 5 times.
That is, we partition the data set into 10 parts and use 9 parts to train the model and the last to evaluate the model.
Several metrics are computed based on the results such as $F_1$ score, area under the ROC curve (ROC-AUC), area under the precision-recall curve, accuracy, sensitivity, and specificity.
This process is repeated so that each part of the partition is used to validate exactly once.
We repeat this entire process 5 times so that 10 random partitions are generated.
This results in a distribution for the metrics which is an approximation for how to models will perform out of sample.
For each of the models we performed a grid-search to find the optimal parameters.
The area under the precision-recall curve was used as the (threshold invariant) metric for which the parameters were tuned.

The variables in \@ref(tab:variable-set) were used as the initial set of variables for `ProgressedToTreatment` and `Prostatectomy`. In the case of `BiopsyUpgraded`, the variable `BiopsyResult` was removed from the predictor variables.
```{r variable-set}
var.names <- models$ProgressedToTreatment$gbm$finalModel$var.names;
mid <- ceiling(length(var.names) / 2);
# Add "" if variable lengths are not even
var2 <- rep("", mid);
var2[1:(mid - 1)] <- var.names[(mid + 1):length(var.names)];
var.names.df <- data.frame(
  var1 = var.names[1:mid],
  var2 = var2
)

kable(
  var.names.df,
  col.names = NULL,
  booktabs = T,
  caption = 'Variables used to predict ProgressedToTreatment and Prostatectomy') %>%
  kable_styling(position =  'center') %>%
  add_header_above(c('Variables' = 2), align = 'c')
```

### ROC and PR Curves
For each of the targets, `ProgressedToTreatment`, `BiopsyUpgraded`, and `Prostatectomy` we construct Receiver operating characteristic (ROC) and Precision-Recall curves.
The sensitivity/recall, specificity, and precision are all computed from the cross-validation folds to give an estimate of out of sample performance.
```{r roc-curves, out.width="90%", fig.cap="Cross-Validation ROC Curves for Progressed to Treatment and Biopsy Upgraded"}
cex.main <- 1.2
cex.title <- 1.4

models.roc <- lapply(models, function(mods) {
  lapply(mods, function(m) {
      bestPreds <- with(m, merge(pred, bestTune));
      pROC::roc(predictor = bestPreds$yes, response = bestPreds$obs, direction = '<', levels = c('no', 'yes'));
  })
})
```

```{r prog-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Progressed to Treatment. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$ProgressedToTreatment, cex.main = cex.main)
mtext('Progressed to Treatment Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

```{r bx-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Biopsy Upgraded. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$BiopsyUpgraded, cex.main = cex.main)
mtext('Biopsy Upgraded Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

```{r prostate-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Prostatectomy. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$Prostatectomy, cex.main = cex.main)
mtext('Prostatectomy Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

### Optimal Operating Point
```{r, cache=TRUE}
# cost.matrix <- lapply(1:3, function(x) matrix(c(0,x,1,0), byrow = TRUE, nrow = 2))
# cost.matrix <- matrix(c(0,3,1,0), byrow = TRUE, nrow = 2)
cost.matrix <- matrix(c(0,2,1,0), byrow = TRUE, nrow = 2)
thresholds <- seq(.01, .99, length.out = 50)

statistics <- c("prob_threshold", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", 
"Precision", "Recall", "F1", "Prevalence", "Detection Rate", 
"Detection Prevalence", "Balanced Accuracy", "Accuracy", "Kappa"
)

optimal.thresholds <- lapply(models, function(mods) {
  lapply(mods, function(m) {
      optimal.threshold.train(m, thresholds = thresholds, cost.matrix = cost.matrix)
  })
})

# Use the ROC levels instead
optimal.thresholds.youden <- lapply(models.roc, function(rocs) {
  lapply(rocs, function(r) {
      as.numeric(coords(r, 'best')[1])
  })
})

thresholds.info.list <- lapply(names(optimal.thresholds), function(x) {
  target.list <- models[[x]]
  
  res <- lapply(names(target.list), function(y) {
    m <- target.list[[y]]
    threshold <- optimal.thresholds[[x]][[y]]
    threshold.roc <- optimal.thresholds.youden[[x]][[y]]
    
    thres.res <- colMeans(threshold.summary.stats(m, threshold), na.rm = TRUE);
    thres.res$model <- m$method;
    thres.res$method <- 'cost';
    thres.res$target <- x;
    thres.res$threshold <- threshold;
    
    thres.res.roc <- colMeans(threshold.summary.stats(m, threshold.roc), na.rm = TRUE);
    thres.res.roc$model <- m$method;
    thres.res.roc$method <- 'roc';
    thres.res.roc$target <- x;
    thres.res.roc$threshold <- threshold.roc;
    
    rbind(thres.res, thres.res.roc)
  })
  
  do.call(rbind.data.frame, res)
})

names(thresholds.info.list) <- names(optimal.thresholds);

thresholds.info <- do.call(rbind.data.frame, thresholds.info.list)
```

Each of the models outputs a probability being in class 1, e.g. the probability of biopsy upgraded, or progression to treatment.
By default the models threshold anything above 0.5 as class 1 and anything below as 0.
We want to find a better threshold value, called the operating point.

The first method we test for finding the optimal operating point is to maximize Youden's J statistic @Youden1950 which is defined as
$$
J = \text{sensitivity} + \text{specificity} - 1
$$
The second method we use to find the optimal operating point is to make a false negative FN (when we predict indolent disease when it truly is aggressive) 2 times as costly as a false positive FP.
For each threshold the cost is computed for each of the folds in the 10-fold cross-validation and averaged across all 10 folds and 5 repetitions.
Then the optimal operating point is selected as the threshold with the minimum cost.

We show how to find the optimal point for predicting whether a patient progressed to treatment.
```{r, fig.cap="Optimal operating point for the Progressed to Treatment prediction models."}
threshold.cost.gbm <- cost.threshold.train(models$ProgressedToTreatment$gbm, thresholds = thresholds, cost.matrix = cost.matrix)

threshold.cost.rpart <- cost.threshold.train(models$ProgressedToTreatment$rpart, thresholds = thresholds, cost.matrix = cost.matrix)

threshold.cost.xgb <- cost.threshold.train(models$ProgressedToTreatment$xgb, thresholds = thresholds, cost.matrix = cost.matrix)

n.models <- length(models$ProgressedToTreatment);

threshold.plot.df <- rbind(
  data.frame(threshold = thresholds, cost = threshold.cost.gbm, model = "gbm"),
  data.frame(threshold = thresholds, cost = threshold.cost.rpart, model = "rpart"),
  data.frame(threshold = thresholds, cost = threshold.cost.xgb, model = "xgb"))

min.thresholds <- thresholds[c(which.min(threshold.cost.gbm), which.min(threshold.cost.rpart), which.min(threshold.cost.xgb))]
xat <- c(0, sort(min.thresholds), 0.5, 1)

BoutrosLab.plotting.general::create.scatterplot(
  formula = cost ~ threshold,
  groups = threshold.plot.df$model,
  data = threshold.plot.df,
  col = default.colours(n.models),
  xat = xat,
  xaxis.lab = round(xat, 2),
  xaxis.cex = 1,
  yaxis.cex = 1,
  ylab.cex = 1.2,
  xlab.cex = 1.2,
  ylab.label = 'mean cost',
  main = "Progressed to Treatment\nOptimal operating points",
  main.cex = 1.3,
  type = "l",
  lwd = 2,
  abline.v = min.thresholds,
  abline.col = default.colours(n.models),
  key = list(
    text = list(
      lab = names(models$ProgressedToTreatment),
      cex = 1,
      col = 'black'
    ),
    lines = list(
      type = 'l',
      col = default.colours(n.models),
      cex = 1,
      lwd = 2
    ),
    x = 0.8,
    y = 0.2
  )
)
```

The results are very similar for the $J$ statistic optimization and the cost-weighted method.
The only model in which the results differ dramatically are in the prostatectomy prediction.
In the cost-weighted method the threshold is optimized at 0 or 1 which results in all positive or all negative predictions.
Using the $J$ statistic prevents the models from predicting the same class. 
We show summary statistics in \@ref(tab:threshold-results) for the optimal thresholds for both J statistic and cost-weighted methods.
```{r threshold-results, fig.cap='Summary statistics for the optimal operating points for each model'}
cols <- c("target", "model", "method", "threshold", "Accuracy", "Sensitivity", "Specificity", "Precision"
  #"Pos Pred Value", "Neg Pred Value",
  ,"F1"
  )

thresholds.table <- thresholds.info[, cols]
# Break camel case into newlines
thresholds.table$target <- kableExtra::linebreak(camel.to.spaces(thresholds.table$target, replace = "\n"), align = "c")
num.cols <- 4:length(cols)
thresholds.table[, num.cols] <- lapply(thresholds.table[, num.cols], function(x) round(as.numeric(x), 2))

kable(
  thresholds.table,
  digits = 2,
  row.names = FALSE,
  caption = "Summary statistics for the different operating points, targets and models.",
  escape = F) %>%
  kable_styling() %>%
  column_spec(1, width_min = "7em") %>%
  collapse_rows(columns = 1:3)
  
```

## Cut-points for time-to-event analysis
The gradient boosted model (gbm) provided the best predictive power of the models.
We use the predictions from this model to show that days until the patient progressed or had the biopsy upgraded for each of the prediction groups.

### Progressed To Treatment
```{r, fig.cap = 'Days to progression from biopsy for the prediction groups'}
prog.gbm.preds <- ifelse(predict(models$ProgressedToTreatment$gbm, type = 'prob')[, 'yes'] > optimal.thresholds$ProgressedToTreatment$gbm, 1, 0)

progression.surv.bx.valid <- do.call(
    what = survival::Surv,
    args = surv.format(biodb.valid$DaysBxToProgression, biodb.valid$DaysBxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      # Is this right to remove the missing values?
      survival.object = progression.surv.bx.valid[!is.na(progression.surv.bx.valid)],
      patient.groups = prog.gbm.preds[!is.na(progression.surv.bx.valid)],
      main = 'Overall Days-to-Progression from Biopsy',
      xlab.label = 'Time to upgrade since biopsy (Days)'
      )
    )
  )
```

```{r, fig.cap = 'Days to progression from diagnosis for the prediction groups'}
progression.surv.dx.valid <- do.call(
    what = survival::Surv,
    args = surv.format(biodb.valid$DaysDxToProgression, biodb.valid$DaysDxToLastReview)
    );


do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.dx.valid,
      patient.groups = prog.gbm.preds,
      main = 'Days-to-Progression from diagnosis',
      xlab.label = 'Time to progression from diagnosis (Days)'
      )
    )
  )
```

### Days to Upgrade from Diagnosis
```{r}
bx.gbm.preds <- rep(NA, nrow(biodb.valid))
bx.gbm.preds[!is.na(biodb.valid$BiopsyUpgraded)] <- ifelse(predict(models$BiopsyUpgraded$gbm, type = 'prob')[, 'yes'] > optimal.thresholds$BiopsyUpgraded$gbm, 1, 0)
# bx.gbm.preds <- as.factor(bx.gbm.preds)
# levels(bx.gbm.preds) <- c('0', '1')

upgrade.surv.valid <- do.call(
    what = survival::Surv,
    args = surv.format(biodb.valid$DaysDxToUpgrade, biodb.valid$DaysDxToLastReview)
    );


do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = upgrade.surv.valid,
      patient.groups = bx.gbm.preds,
      main = 'Overall Days-to-Upgrade from Diagnosis',
      xlab.label = 'Time to upgrade since diagnosis (Days)'
      )
    )
  )
```


## Variable Importance for Progressed to Treatment
The regression tree output is simple to interpret but does not provide the best predictions.
The GBM is not as easy to interpret but has a measure of variable importance of the predictors.
We compare the models' variable importance ranks to see important variables across the models.

```{r}
var.imp.joined <- compare.var.imp(models$ProgressedToTreatment, include.ranks = TRUE)[, -1]
                   
var.imp.joined <- var.imp.joined[order(var.imp.joined$mean.rank),]
kable(
  x = head(var.imp.joined[, c('mean.rank', 'gbm.rank', 'xgb.rank', 'rpart.rank')], n = 25),
  caption = paste('Top 25 variable importance rankings out of', nrow(var.imp.joined), 'variables'), digits = 2,
  col.names = c('Mean rank', 'GBM rank', 'XGB rank', 'rpart rank')
  )
```

## Decision Trees
The GBM models perform much better than the rpart models.
We include the rpart decision tree models as they are easy to interpret. 

```{r decision-tree-bx, out.width="75%", fig.cap="Decision tree for Biopsy Upgraded prediction."}
rpart.plot(models$BiopsyUpgraded$rpart$finalModel);
```

```{r decision-tree-prog, out.width="90%", fig.cap="Decision tree for Progressed to Treatment prediction."}
rpart.plot(models$ProgressedToTreatment$rpart$finalModel);
```

```{r decision-tree-prostate, out.width="75%", fig.cap="Decision tree for Prostatectomy prediction."}
rpart.plot(models$Prostatectomy$rpart$finalModel);
```

# Next steps

- Describe the following analysis steps
- Propose solutions to address constraints or opportunities uncovered during the analysis

# Conclusion/Discussion

- Optional
- Describe the patterns, principles, and relationships shown by each major findings
- Describe potential limitations and/or new questions

# Methods

-  The tools/software you are using and cite version of software and packages used
    - R language and environment (R Core Team 2019, v3.6.0)
    - BPG (v5.9.8) [@BPG] (to embed citation within text)
- Describe handling dropouts and missing data

```{r reproducibility, echo = FALSE}
## Reproducibility info
options(width = 120)
sessioninfo::session_info()
```

\newpage

# References

<div id="refs"></div>

\newpage

# Appendix

- Optional
- Include supplementary materials that may be helpful in providing a more comprehensive understanding of the research problem.
- Technical description of statistical procedures
- Detailed tables or computer output
- Figures that were not central to the arguments presented in the body of the reportot.
