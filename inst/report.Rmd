---
title: 'Active Surveillance Progression Prediction'
author: 'Stefan Eng'
output:
  bookdown::pdf_document2:
    number_sections: false
    keep_tex: true
  html_notebook: default
bibliography: bibliography.bib
csl: acm-sig-proceedings.csl
---

```{r setup, include=FALSE}
library(ProstateCancer.ASBiomarkerSynergy);
library(BoutrosLab.plotting.general);
library(kableExtra);
library(gridExtra);
library(magrittr);
library(here);
library(caret);
# library(MLeval);
library(pROC)
library(rpart.plot);

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, fig.align = 'center', out.width = '70%')

axis.cex <- .7
lab.cex <- .8
strip.cex <- 0.45

km.arguments <- list(
  xaxis.cex = axis.cex,
  yaxis.cex = axis.cex,
  main.cex = 1.4,
  xlab.cex = lab.cex,
  ylab.label = 'Estimated survival probability',
  ylab.cex = lab.cex,
  risktable.fontsize = 14,
  key.groups.cex = 1,
  key.stats.cex = 1,
  key.stats.y.pos = 0.5
  );

biodb <- default.load.data(onlyBiodb = TRUE);
```

## Data
In this cohort there are $N = `r nrow(biodb)`$ patients.
There are three variables that are of interest to predict.
The first, `BiopsyUpgraded`, is whether the patient's research study biopsy increased cancer grade from the most recent biopsy results.
The second, `ProgressedToTreatment`, is whether the patient progressed to treatment.
Finally we have `Prostatectomy` which indicates whether the patient had a prostatectomy.

```{r}
pred.vars.xtabs <- xtabs(~ BiopsyUpgraded + ProgressedToTreatment + Prostatectomy, data = biodb, addNA = TRUE)
pred.vars.df <- as.data.frame(ftable(pred.vars.xtabs))

pred.table <- reshape(pred.vars.df, idvar = c('BiopsyUpgraded', 'ProgressedToTreatment'), timevar = 'Prostatectomy', direction = 'wide')

row.order <- with(pred.table, order(BiopsyUpgraded, ProgressedToTreatment))
prostatectomy.cols <- paste0(0:1, ' (N=', table(biodb$Prostatectomy), ')')
overall.col <- paste0('Overall (N=', nrow(biodb), ')')

pred.table$Overall <- rowSums(pred.table[row.order, c(3,4)])

pred.table[row.order, ] %>%
  kable(row.names = FALSE,
    booktabs = TRUE,
    align =  c('c', 'c', 'c', 'c'),
    col.names = c('Biopsy Upgraded', 'Progressed To Treatment', prostatectomy.cols, overall.col),
    caption = 'Comparison of prediction variables of interest') %>%
  kable_styling() %>%
  collapse_rows(columns = 1) %>%
  add_header_above(c(' ' = 2, 'Prostatectomy' = 2, ' ' = 1), align = 'c')
```

The severity of the prostate cancers were measured on the Gleason scale.
The variable `PreviousGleason` is the most recent prostate cancer Gleason score rating prior to research biopsy.
The Gleason score is computed again in the variable `StudyHighestGleason` after the research MRI.
We compare the progression of the cancers by the Gleason grades.
```{r}
progressed <- rep(NA, length(biodb$PreviousGleason))
progressed[biodb$PreviousGleason < biodb$StudyHighestGleason] <- 'Increased';
progressed[biodb$PreviousGleason > biodb$StudyHighestGleason] <- 'Decreased';
progressed[biodb$PreviousGleason == biodb$StudyHighestGleason] <- 'No Change';

kable(t(table(progressed)), caption = 'Comparison of pre-research biopsy Gleason score to post research MRI Gleason score');

with(biodb, {
  table(PreviousGleason, StudyHighestGleason, useNA='ifany') %>%
  kable(booktabs = TRUE,
    align =  'c') %>%
  kable_styling() %>%
  add_header_above(c(' ' = 1, 'Study Highest Gleason' = length(levels(StudyHighestGleason)) + 1), align = 'c')
})
```

### Demographics

#### Biopsy Upgraded
```{r, results= 'hide', out.width='90%'}
demographic.vars <- c('Age', 'Weight', 'Height', 'BMI');
bx.plots <- lapply(demographic.vars, demographics.boxplot, biodb = biodb, x = 'BiopsyUpgraded', cond = 'Race', yaxis.cex = axis.cex, xaxis.cex = axis.cex, ylab.cex = lab.cex, xlab.cex = lab.cex, strip.cex = strip.cex);
do.call(gridExtra::grid.arrange, bx.plots);
```

#### Progressed to Treatment
```{r, results= 'hide', out.width='90%'}
demographic.vars <- c('Age', 'Weight', 'Height', 'BMI');
prog.plots <- lapply(demographic.vars, demographics.boxplot, biodb = biodb, x = 'ProgressedToTreatment', cond = 'Race', yaxis.cex = axis.cex, xaxis.cex = axis.cex, ylab.cex = lab.cex, xlab.cex = lab.cex, strip.cex = strip.cex);
do.call(gridExtra::grid.arrange, prog.plots);
```

### Table Summaries
```{r}
# TODO: Update render.default to always include missing?
t1 <- table1::table1(~ p2PSA +  freePSA + PSAHyb + PSADensity + SOCPSA + PHI + PHIDensity | ProgressedToTreatment, data = biodb);
table1.to.kable(t1) %>%
  add_header_above(c(' ' = 1, 'Progressed To Treatment' = 2, ' ' = 1))
```

```{r}
t1.bx <- table1::table1(~ p2PSA +  freePSA + PSAHyb + PSADensity + SOCPSA + PHI + PHIDensity | BiopsyUpgraded, data = biodb);
table1.to.kable(t1.bx) %>%
  add_header_above(c(' ' = 1, 'Biopsy Upgraded' = 2, ' ' = 1))
```

### Correlation Heatmap
```{r, out.width = '100%'}
create.heatmap.AS(biodb);
```

### Time-to-event data

```{r}
stopifnot(
    all(!is.na(biodb$DaysDxToLastClinicalAppt))
    );
stopifnot(
    all(!is.na(biodb$DaysDxToLastReview))
    );
stopifnot(
    all(is.na(biodb$BiopsyResult) == is.na(biodb$DaysBxToLastReview))
    );
stopifnot(
    all(is.na(biodb$BiopsyResult) == is.na(biodb$DaysBxToLastClinicalAppt))
    );
```
All patients have values for the days from diagnosis to the last clinical appointment `DaysDxToLastClinicalAppt` and last review `DaysDxToLastReview`.
The patients that had a biopsy have values for variables `DaysBxToLastClinicalAppt` and `DaysBxToLastReview`.
If a patient progressed to treatment, then the days from diagnosis are in `DaysDxToProgression`.
If they had a biopsy and progressed to treatment then `DaysDxToProgression` will contain the days between the biopsy and progression.

#### Overall Days to Progression from Biopsy/Diagnosis
```{r}
# Create a Surv object from survival package
progression.surv.bx <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysBxToProgression, biodb$DaysBxToLastReview)
    );

progression.surv.dx <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysDxToProgression, biodb$DaysDxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.bx,
      main = 'Overall Days-to-Progression from Biopsy',    
      xlab.label = 'Time to progression from biopsy (Days)'
      )
    )
  )

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.dx,
      main = 'Overall Days-to-Progression from Diagnosis',
      xlab.label = 'Time to progression from diagnosis (Days)'
      )
    )
  )
```

#### Overall Time-to-Upgrade from Diagnosis
```{r}
# Create a Surv object from survival package
upgrade.surv <- do.call(
    what = survival::Surv,
    args = surv.format(biodb$DaysDxToUpgrade, biodb$DaysDxToLastReview)
    );

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = upgrade.surv,
      main = 'Overall Days-to-Upgrade from Diagnosis',
      xlab.label = 'Time to upgrade since diagnosis (Days)'
      )
    )
  )
```

## Predictive Analysis
```{r}
seed <- 1313;
model.names <- c('rpart', 'gbm');
targets <- c(
    'BiopsyUpgraded',
    'Prostatectomy',
    'ProgressedToTreatment');
metric <- 'PR-AUC';

models <- lapply(targets, function(tg) {
    model.file.names <- here(paste0('models/', model.names, '_', tg, '_', metric, '_', seed, '_model.RDS'));
    res <- lapply(model.file.names, readRDS);
    names(res) <- model.names;
    res
});
names(models) <- targets;
```


### Method

We predict the three variables: `ProgressedToTreatment`, `BiopsyUpgraded`, `Prostatectomy`.
For each of these variable we compare the results from three models: a regression tree (`rpart`), gradient boosting machine (`gbm`), and eXtreme Gradient Boosting (`xgboost`).
We only report the results from `rpart` and `gbm` as the results are very similar for the two gradient boosting methods and `gbm` performs slightly better.
The regression tree model is the most interpretable as it produces a single decision tree.
The other models produce better accuracy as well as $F_1$ scores but require more work to interpret.

All of the models were validated using 10-fold cross-validation repeated 5 times.
That is, we partition the data set into 10 parts and use 9 parts to train the model and the last to evaluate the model.
Several metrics are computed based on the results such as $F_1$ score, area under the ROC curve (ROC-AUC), area under the precision-recall curve, accuracy, sensitivity, and specificity.
This process is repeated so that each part of the partition is used to validate exactly once.
We repeat this entire process 5 times so that 10 random partitions are generated.
This results in a distribution for the metrics which is an approximation for how to models will perform out of sample.
For each of the models we performed a grid-search to find the optimal parameters.
The area under the precision-recall curve was used as the (threshold invariant) metric for which the parameters were tuned.

The variables in \@ref(tab:variable-set) were used as the initial set of variables for `ProgressedToTreatment` and `Prostatectomy`. In the case of `BiopsyUpgraded`, the variable `BiopsyResult` was removed from the predictor variables.
```{r variable-set}
var.names <- models$ProgressedToTreatment$gbm$finalModel$var.names;
mid <- ceiling(length(var.names) / 2);
# Add "" if variable lengths are not even
var2 <- rep("", mid);
var2[1:(mid - 1)] <- var.names[(mid + 1):length(var.names)];
var.names.df <- data.frame(
  var1 = var.names[1:mid],
  var2 = var2
)

kable(
  var.names.df,
  col.names = NULL,
  booktabs = T,
  caption = 'Variables used to predict ProgressedToTreatment and Prostatectomy') %>%
  kable_styling(position =  'center') %>%
  add_header_above(c('Variables' = 2), align = 'c')
```

### ROC and PR Curve
For each of the targets, `ProgressedToTreatment`, `BiopsyUpgraded`, and `Prostatectomy` we construct Receiver operating characteristic (ROC) and Precision-Recall curves.
The sensitivity/recall, specificity, and precision are all computed from the cross-validation folds to give an estimate of out of sample performance.
```{r roc-curves, out.width="90%", fig.cap="Cross-Validation ROC Curves for Progressed to Treatment and Biopsy Upgraded"}
cex.main <- 1.2
cex.title <- 1.4

models.roc <- lapply(models, function(mods) {
  lapply(mods, function(m) {
      bestPreds <- with(m, merge(pred, bestTune));
      pROC::roc(predictor = bestPreds$yes, response = bestPreds$obs, direction = '<', levels = c('no', 'yes'));
  })
})
```

```{r prog-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Progressed to Treatment. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$ProgressedToTreatment, cex.main = cex.main)
mtext('Progressed to Treatment Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

```{r bx-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Biopsy Upgraded. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$BiopsyUpgraded, cex.main = cex.main)
mtext('Biopsy Upgraded Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

```{r prostate-roc, out.width="90%", fig.cap="Cross-Validation ROC and Precision-Recall Curves for Prostatectomy. The optimal (Youden's J statistic) is indicated with a point on the ROC curve."}
roc.pr.plot(models.roc$Prostatectomy, cex.main = cex.main)
mtext('Prostatectomy Cross-Validation Curves', side = 3, line = -2, outer = TRUE, cex = cex.title);
```

### Optimal Operating Point
```{r, cache=TRUE}
# cost.matrix <- lapply(1:3, function(x) matrix(c(0,x,1,0), byrow = TRUE, nrow = 2))
# cost.matrix <- matrix(c(0,3,1,0), byrow = TRUE, nrow = 2)
cost.matrix <- matrix(c(0,2,1,0), byrow = TRUE, nrow = 2)
thresholds <- seq(.01, .99, length.out = 100)

statistics <- c("prob_threshold", "Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", 
"Precision", "Recall", "F1", "Prevalence", "Detection Rate", 
"Detection Prevalence", "Balanced Accuracy", "Accuracy", "Kappa"
)

optimal.thresholds <- lapply(models, function(mods) {
  lapply(mods, function(m) {
      optimal.threshold.train(m, thresholds = thresholds, cost.matrix = cost.matrix)
  })
})

# Use the ROC levels instead
optimal.thresholds.youden <- lapply(models.roc, function(rocs) {
  lapply(rocs, function(r) {
      as.numeric(coords(r, 'best')[1])
  })
})

thresholds.info.list <- lapply(names(optimal.thresholds), function(x) {
  target.list <- models[[x]]
  
  res <- lapply(names(target.list), function(y) {
    m <- target.list[[y]]
    threshold <- optimal.thresholds[[x]][[y]]
    threshold.roc <- optimal.thresholds.youden[[x]][[y]]
    
    thres.res <- colMeans(threshold.summary.stats(m, threshold), na.rm = TRUE);
    thres.res$model <- m$method;
    thres.res$method <- 'cost';
    thres.res$target <- x;
    thres.res$threshold <- threshold;
    
    thres.res.roc <- colMeans(threshold.summary.stats(m, threshold.roc), na.rm = TRUE);
    thres.res.roc$model <- m$method;
    thres.res.roc$method <- 'roc';
    thres.res.roc$target <- x;
    thres.res.roc$threshold <- threshold.roc;
    
    rbind(thres.res, thres.res.roc)
  })
  
  do.call(rbind.data.frame, res)
})

names(thresholds.info.list) <- names(optimal.thresholds);

thresholds.info <- do.call(rbind.data.frame, thresholds.info.list)
```

Each of the models outputs a probability being in class 1, e.g. the probability of biopsy upgraded, or progression to treatment.
By default the models threshold anything above 0.5 as class 1 and anything below as 0.
We want to find a better threshold value, called the operating point.

The first method we test for finding the optimal operating point is to maximize Youden's J statistic @Youden1950 which is defined as
$$
J = \text{sensitivity} + \text{specificity} - 1
$$
The second method we use to find the optimal operating point is to make a false negative FN (when we predict indolent disease when it truly is aggressive) 2 times as costly as a false positive FP.
For each threshold the cost is computed for each of the folds in the 10-fold cross-validation and averaged across all 10 folds and 5 repetitions.
Then the optimal operating point is selected as the threshold with the minimum cost.

We show how to find the optimal point for predicting whether a patient progressed to treatment.
```{r, fig.cap="Optimal operating point for the Progressed to Treatment prediction models."}
threshold.cost.gbm <- cost.threshold.train(models$ProgressedToTreatment$gbm, thresholds = thresholds, cost.matrix = cost.matrix)

threshold.cost.rpart <- cost.threshold.train(models$ProgressedToTreatment$rpart, thresholds = thresholds, cost.matrix = cost.matrix)

threshold.plot.df <- rbind(
  data.frame(threshold = thresholds, cost = threshold.cost.gbm, model = "gbm"),
  data.frame(threshold = thresholds, cost = threshold.cost.rpart, model = "rpart"))

min.thresholds <- thresholds[c(which.min(threshold.cost.gbm), which.min(threshold.cost.rpart))]
xat <- c(0, sort(min.thresholds), 0.5, 1)

BoutrosLab.plotting.general::create.scatterplot(
  formula = cost ~ threshold,
  groups = threshold.plot.df$model,
  data = threshold.plot.df,
  col = default.colours(2),
  xat = xat,
  xaxis.lab = round(xat, 2),
  xaxis.cex = 1,
  yaxis.cex = 1,
  ylab.cex = 1.2,
  xlab.cex = 1.2,
  ylab.label = 'mean cost',
  main = "Progressed to Treatment\nOptimal operating points",
  main.cex = 1.3,
  type = "l",
  lwd = 2,
  abline.v = min.thresholds,
  abline.col = default.colours(2),
  key = list(
    text = list(
      lab = c('GBM', 'rpart'),
      cex = 1,
      col = 'black'
    ),
    lines = list(
      type = 'l',
      col = default.colours(2),
      cex = 1,
      lwd = 2
    ),
    x = 0.8,
    y = 0.2
  )
)
```

The results are very similar for the $J$ statistic optimization and the cost-weighted method.
The only model in which the results differ dramatically are in the prostatectomy prediction.
In the cost-weighted method the threshold is optimized at 0 or 1 which results in all positive or all negative predictions.
Using the $J$ statistic prevents the models from predicting the same class. 
We show summary statistics in \@ref(tab:threshold-results) for the optimal thresholds for both J statistic and cost-weighted methods.
```{r threshold-results, fig.cap='Summary statistics for the optimal operating points for each model'}
cols <- c("target", "model", "method", "threshold", "Accuracy", "Sensitivity", "Specificity", "Precision"
  #"Pos Pred Value", "Neg Pred Value",
  ,"F1"
  )

thresholds.table <- thresholds.info[, cols]
# Break camel case into newlines
thresholds.table$target <- kableExtra::linebreak(camel.to.spaces(thresholds.table$target, replace = "\n"), align = "c")
num.cols <- 4:length(cols)
thresholds.table[, num.cols] <- lapply(thresholds.table[, num.cols], function(x) round(as.numeric(x), 2))

kable(
  thresholds.table,
  digits = 2,
  row.names = FALSE,
  caption = "Summary statistics for the different operating points, targets and models.",
  escape = F) %>%
  kable_styling() %>%
  column_spec(1, width_min = "7em") %>%
  collapse_rows(columns = 1:3)
  
```

## Cut-points for time-to-event analysis
The gradient boosted model (gbm) provided the best predictive power of the models.
We use the predictions from this model to show that days until the patient progressed or had the biopsy upgraded for each of the prediction groups.

### Progressed To Treatment
```{r}
prog.gbm.preds <- ifelse(predict(models$ProgressedToTreatment$gbm, type = 'prob')[, 'yes'] > optimal.thresholds$ProgressedToTreatment$gbm, 1, 0)

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      # Is this right to remove the missing values?
      survival.object = progression.surv.bx[!is.na(progression.surv.bx)],
      patient.groups = prog.gbm.preds[!is.na(progression.surv.bx)],
      main = 'Overall Days-to-Progression from Biopsy',
      xlab.label = 'Time to upgrade since biopsy (Days)'
      )
    )
  )

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = progression.surv.dx,
      patient.groups = prog.gbm.preds,
      main = 'Overall Days-to-Progression from Diagnosis',
      xlab.label = 'Time to progression from diagnosis (Days)'
      )
    )
  )
```

### Days to Upgrade from Diagnosis
```{r}
bx.gbm.preds <- rep(NA, nrow(biodb))
bx.gbm.preds[!is.na(biodb$BiopsyUpgraded)] <- ifelse(predict(models$BiopsyUpgraded$gbm, type = 'prob')[, 'yes'] > optimal.thresholds$BiopsyUpgraded$gbm, 1, 0)
# bx.gbm.preds <- as.factor(bx.gbm.preds)
# levels(bx.gbm.preds) <- c('0', '1')

do.call(
  what = BoutrosLab.plotting.survival::create.km.plot,
  args = c(
    km.arguments,
    list(
      survival.object = upgrade.surv,
      patient.groups = bx.gbm.preds,
      main = 'Overall Days-to-Upgrade from Diagnosis',
      xlab.label = 'Time to upgrade since diagnosis (Days)'
      )
    )
  )
```


## Variable Importance for Progressed to Treatment
The regression tree output is simple to interpret but does not provide the best predictions.
The GBM is not as easy to interpret but has a measure of variable importance of the predictors.
We compare the models' variable importance ranks to see important variables across the models.

```{r}
var.imp.joined <- compare.var.imp(models$ProgressedToTreatment, include.ranks = TRUE)[, -1]
                   
var.imp.joined <- var.imp.joined[order(var.imp.joined$mean.rank),]
kable(
  x = head(var.imp.joined[, c('mean.rank', 'gbm.rank', 'rpart.rank')], n = 25),
  caption = paste('Top 25 variable importance rankings out of', nrow(var.imp.joined), 'variables'), digits = 2,
  col.names = c('Mean rank', 'rpart rank', 'GBM rank')
  )
```

## Decision Trees
The GBM models perform much better than the rpart models.
We include the rpart decision tree models as they are easy to interpret. 

```{r decision-tree-bx, out.width="75%", fig.cap="Decision tree for Biopsy Upgraded prediction."}
rpart.plot(models$BiopsyUpgraded$rpart$finalModel);
```

```{r decision-tree-prog, out.width="90%", fig.cap="Decision tree for Progressed to Treatment prediction."}
rpart.plot(models$ProgressedToTreatment$rpart$finalModel);
```

```{r decision-tree-prostate, out.width="75%", fig.cap="Decision tree for Prostatectomy prediction."}
rpart.plot(models$Prostatectomy$rpart$finalModel);
```

\newpage

## References
